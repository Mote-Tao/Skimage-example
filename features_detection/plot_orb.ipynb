{
  "metadata": {
    "language_info": {
      "name": "python",
      "nbconvert_exporter": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython3",
      "version": "3.4.2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "nbformat_minor": 0,
  "cells": [
    {
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "execution_count": null,
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "\n# ORB feature detector and binary descriptor\n\n\nThis example demonstrates the ORB feature detection and binary description\nalgorithm. It uses an oriented FAST detection method and the rotated BRIEF\ndescriptors.\n\nUnlike BRIEF, ORB is comparatively scale and rotation invariant while still\nemploying the very efficient Hamming distance metric for matching. As such, it\nis preferred for real-time applications.\n\n\n"
      ]
    },
    {
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "execution_count": null,
      "cell_type": "code",
      "source": [
        "from skimage import data\nfrom skimage import transform as tf\nfrom skimage.feature import (match_descriptors, corner_harris,\n                             corner_peaks, ORB, plot_matches)\nfrom skimage.color import rgb2gray\nimport matplotlib.pyplot as plt\n\n\nimg1 = rgb2gray(data.astronaut())\nimg2 = tf.rotate(img1, 180)\ntform = tf.AffineTransform(scale=(1.3, 1.1), rotation=0.5,\n                           translation=(0, -200))\nimg3 = tf.warp(img1, tform)\n\ndescriptor_extractor = ORB(n_keypoints=200)\n\ndescriptor_extractor.detect_and_extract(img1)\nkeypoints1 = descriptor_extractor.keypoints\ndescriptors1 = descriptor_extractor.descriptors\n\ndescriptor_extractor.detect_and_extract(img2)\nkeypoints2 = descriptor_extractor.keypoints\ndescriptors2 = descriptor_extractor.descriptors\n\ndescriptor_extractor.detect_and_extract(img3)\nkeypoints3 = descriptor_extractor.keypoints\ndescriptors3 = descriptor_extractor.descriptors\n\nmatches12 = match_descriptors(descriptors1, descriptors2, cross_check=True)\nmatches13 = match_descriptors(descriptors1, descriptors3, cross_check=True)\n\nfig, ax = plt.subplots(nrows=2, ncols=1)\n\nplt.gray()\n\nplot_matches(ax[0], img1, img2, keypoints1, keypoints2, matches12)\nax[0].axis('off')\nax[0].set_title(\"Original Image vs. Transformed Image\")\n\nplot_matches(ax[1], img1, img3, keypoints1, keypoints3, matches13)\nax[1].axis('off')\nax[1].set_title(\"Original Image vs. Transformed Image\")\n\n\nplt.show()"
      ]
    }
  ],
  "nbformat": 4
}